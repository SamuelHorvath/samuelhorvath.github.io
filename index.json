[{"authors":["admin"],"categories":null,"content":"I am the third year student of MS/PhD program in Machine Learning and Optimization under supervision of professor Peter Richtárik at Visual Computing Center KAUST. Prior to that, I studied Financial Mathematics at Comenius University. My research interest is mainly in Non-convex and Convex Optimization, especially different Machine Learning and Deep Learning applications.\nOn the other hand, I am also active in sports, mostly football and fitness.\n","date":1612828800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1612828800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://samuelhorvath.github.io/author/samuel-horvath/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/samuel-horvath/","section":"authors","summary":"I am the third year student of MS/PhD program in Machine Learning and Optimization under supervision of professor Peter Richtárik at Visual Computing Center KAUST. Prior to that, I studied Financial Mathematics at Comenius University.","tags":null,"title":"Samuel Horvath","type":"authors"},{"authors":["Samuel Horvath","Stefanos Laskaridis","Mario Almeida","Ilias Leontiadis","Stylianos Venieris","Nicholas Lane"],"categories":null,"content":"","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"1d093a2e8ac9f5ac7c19e45922e366f0","permalink":"https://samuelhorvath.github.io/publication/fjord/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/publication/fjord/","section":"publication","summary":"Federated Learning (FL) has been gaining significant traction across different ML tasks, ranging from vision to keyboard predictions. In large scale deployments, client heterogeneity is a fact, and constitutes a primary problem for fairness, significant efforts have been made into tackling statistical data heterogeneity, the diversity in the clients, termed as system heterogeneity, has remained largely unexplored. Current solutions either disregard a large portion of available devices or set a uniform limit on the model’s capacity, restricted by the least capable participants. In this work, we introduce Ordered Dropout, a mechanism that achieves an ordered, nested representation of knowledge in Neural Networks and enables the extraction of lower footprint submodels without the need of retraining. We further show that for linear maps our Ordered Dropout is equivalent to SVD. We employ this technique, along with a self-distillation methodology, in the realm of FL in a framework called FjORD. FjORD alleviates the problem of client system heterogeneity by tailoring the model width to the client’s capabilities. Extensive evaluation on both CNNs and RNNs across diverse modalities shows that FjORD consistently leads to significant performance gains over state-of-the-art baselines, while maintaining its nested structure.","tags":null,"title":"FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout is now available on the arXiv. This is a joint work with Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nic Lane from Samsung AI Centre, Cambridge, UK.\nAbstract:\nFederated Learning (FL) has been gaining significant traction across different ML tasks, ranging from vision to keyboard predictions. In large scale deployments, client heterogeneity is a fact, and constitutes a primary problem for fairness, significant efforts have been made into tackling statistical data heterogeneity, the diversity in the clients, termed as system heterogeneity, has remained largely unexplored. Current solutions either disregard a large portion of available devices or set a uniform limit on the model’s capacity, restricted by the least capable participants. In this work, we introduce Ordered Dropout, a mechanism that achieves an ordered, nested representation of knowledge in Neural Networks and enables the extraction of lower footprint submodels without the need of retraining. We further show that for linear maps our Ordered Dropout is equivalent to SVD. We employ this technique, along with a self-distillation methodology, in the realm of FL in a framework called FjORD. FjORD alleviates the problem of client system heterogeneity by tailoring the model width to the client’s capabilities. Extensive evaluation on both CNNs and RNNs across diverse modalities shows that FjORD consistently leads to significant performance gains over state-of-the-art baselines, while maintaining its nested structure.\n","date":1614459600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614459600,"objectID":"9537d9281d6929cedd6098662fc3932f","permalink":"https://samuelhorvath.github.io/post/new_paper_out_fjord/","publishdate":"2021-02-28T00:00:00+03:00","relpermalink":"/post/new_paper_out_fjord/","section":"post","summary":"[**FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout**](https://arxiv.org/pdf/2102.13451.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"Our paper A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning, joint work with Peter Richtarik, won the Best Paper Award at NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning. This prize comes with a generous check of 1888$.\nAward ","date":1612828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612828800,"objectID":"1040c7a66aa4a10f74a401a0563dd3ba","permalink":"https://samuelhorvath.github.io/post/neurips2020best_paper/","publishdate":"2021-02-09T00:00:00Z","relpermalink":"/post/neurips2020best_paper/","section":"post","summary":"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning","tags":[],"title":"The Best Paper Award at NeurIPS 2020 SpicyFL Workshop","type":"post"},{"authors":["Samuel Horvath","Aaron Klein","Peter Richtarik","Cedric Archambeau"],"categories":null,"content":"","date":1611360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611360000,"objectID":"65d5d46905911fa3b1e8adfba2a70689","permalink":"https://samuelhorvath.github.io/publication/abrac/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/publication/abrac/","section":"publication","summary":"Bayesian optimization (BO) is a sample efficient approach to automatically tune the hyperparameters of machine learning models. In practice, one frequently has to solve similar hyperparameter tuning problems sequentially. For example, one might have to tune a type of neural network learned across a series of different classification problems. Recent work on multi-task BO exploits knowledge gained from previous tuning tasks to speed up a new tuning task. However, previous approaches do not account for the fact that BO is a sequential decision making procedure. Hence, there is in general a mismatch between the number of evaluations collected in the current tuning task compared to the number of evaluations accumulated in all previously completed tasks. In this work, we enable multi-task BO to compensate for this mismatch, such that the transfer learning procedure is able to handle different data regimes in a principled way. We propose a new multi-task BO method that learns a set of ordered, nonlinear basis functions of increasing complexity via nested drop-out and automatic relevance hyperparameter tuning problems show that our method improves the sample efficiency of recently published multi-task BO methods.","tags":null,"title":"Hyperparameter Transfer Learning with Adaptive Complexity","type":"publication"},{"authors":["Samuel Horvath"],"categories":[],"content":"Our paper [Hyperparameter Transfer Learning with Adaptive Complexity], joint work with Aaron Klein, Peter Richtarik and Cedric Archambeau, got accepted to the AISTATS 2021. This year AISTATS will be fully virtual taking place in April.\n","date":1611360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611360000,"objectID":"a5a27070c28b9435b715706b378f37c4","permalink":"https://samuelhorvath.github.io/post/aistats2021accepted/","publishdate":"2021-01-23T00:00:00Z","relpermalink":"/post/aistats2021accepted/","section":"post","summary":"Hyperparameter Transfer Learning with Adaptive Complexity","tags":[],"title":"One paper accepted to AISTATS 2021","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"Our paper A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning, joint work with Peter Richtarik, got accepted to the ICLR 2021. This year ICLR will be virtual taking place in May.\n","date":1610409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610409600,"objectID":"4bd70fb2f16c7144ed2d1b75487fff56","permalink":"https://samuelhorvath.github.io/post/iclr2021accepted/","publishdate":"2021-01-12T00:00:00Z","relpermalink":"/post/iclr2021accepted/","section":"post","summary":"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning","tags":[],"title":"One paper accepted to ICLR 2021","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"Our paper Optimal Client Sampling for Federated Learning, joint work with Wenlin Chen and Peter Richtarik, was accepted to Privacy Preserving Machine Learning workshop. In addition, our work Adaptivity of Stochhastic Gradient Methods for Nonconvex Optimization, joint work with Michael I. Jordan, Lihua Lei, and Peter Richtarik, was accepted to Optimization for Machine Learning workshop as a Spotlight talk. Lastly, A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning, joint work with Peter Richtarik, and On Biased Compression for Distributed Learning, joint work with Aleksandr Beznosikov, Mher Safaryan and Peter Richtarik, were selected as contributed talks at Workshop on Scalability, Privacy, and Security in Federated Learning.\n UPDATE A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning won the Best Paper Award at NeurIPS -SpicyFL 2020 - NeurIPS-20 Workshop on Scalability, Privacy, and Security in Federated Learning.\n","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"0227fdb4adde1c0a44a123345c6f1978","permalink":"https://samuelhorvath.github.io/post/neurips2020workshops/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/post/neurips2020workshops/","section":"post","summary":"Privacy Preserving Machine Learning, Optimization for Machine Learning  and Workshop on Scalability, Privacy, and Security in Federated Learning","tags":[],"title":"4 papers accepted to NeurIPS 2020 Workshops, 1 Spotlight and 2 Oral Presentations","type":"post"},{"authors":[],"categories":null,"content":"Our new paper Optimal Client Sampling for Federated Learning is now available on the arXiv. This is the joint work with Wenlin Chen and Peter Richtarik.\nAbstract:\nIt is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients allowed to communicate their updates back to the master node. In each communication round, all participated clients compute their updates, but only the ones with \u0026ldquo;important\u0026rdquo; updates communicate back to the master. We show that importance can be measured using only the norm of the update and we give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that our approach leads to superior performance for Distributed SGD (DSGD) and Federated Averaging (FedAvg) compared to the baseline where participating clients are sampled uniformly. Finally, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.\n","date":1603659600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603659600,"objectID":"a15919f535c831379304b9425ce5d901","permalink":"https://samuelhorvath.github.io/post/new_paper_out_floptsampling/","publishdate":"2020-10-26T00:00:00+03:00","relpermalink":"/post/new_paper_out_floptsampling/","section":"post","summary":"[**Optimal Client Sampling for Federated Learning**](https://arxiv.org/abs/2010.13723)","tags":[],"title":"New paper out","type":"post"},{"authors":["Wenlin Chen","Samuel Horvath","Peter Richtarik"],"categories":null,"content":"","date":1603411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603411200,"objectID":"559102fd86b3f2f47e5756a9dd451960","permalink":"https://samuelhorvath.github.io/publication/fl_optimal_sampling/","publishdate":"2020-10-23T00:00:00Z","relpermalink":"/publication/fl_optimal_sampling/","section":"publication","summary":"It is well understood that client-master communication can be a primary bottleneck in Federated Learning. In this work, we address this issue with a novel client subsampling scheme, where we restrict the number of clients  allowed to communicate their updates back to the master node. In each communication round, all participated clients compute their updates, but only the ones with \"important\" updates communicate back to the master. We show that importance can be measured using only the norm of the update and we give a formula for optimal client participation. This formula minimizes the distance between the full update, where all clients participate, and our limited update, where the number of participating clients  is restricted. In addition, we provide a simple algorithm that approximates the optimal formula for client participation which only requires secure aggregation and thus does not compromise client privacy. We show both theoretically and empirically that our approach leads to superior performance for Distributed SGD (DSGD) and Federated Averaging (FedAvg) compared to the baseline where participating clients are sampled uniformly. Finally, our approach is orthogonal to and compatible with existing methods for reducing communication overhead, such as local methods and communication compression methods.","tags":null,"title":"Optimal Client Sampling for Federated Learning","type":"publication"},{"authors":["Samuel Horvath"],"categories":[],"content":"Announcement:\n\u0026ldquo;Hi Samuel,\nThank you for all your hard work reviewing for NeurIPS 2020! We are delighted to inform you that you were in the top 10% of high-scoring reviewers this year! You will therefore be given access to one free registration to this year’s conference; you will later receive additional information by email explaining how to access your registration. If you have already registered, you will receive a refund.\nAll the best, Hsuan-Tien Lin, Maria-Florina Balcan, Raia Hadsell, Marc\u0026rsquo;Aurelio Ranzato NeurIPS 2020 Program Chairs\u0026rdquo;\n","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603238400,"objectID":"3b7f75406160f25592060a840e0731f8","permalink":"https://samuelhorvath.github.io/post/neurips2020reviewer/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/post/neurips2020reviewer/","section":"post","summary":"I have been in the top 10% of high-scoring reviewers.","tags":[],"title":"Among Best Reviewers for NeurIPS 2020","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"Our paper Lower Bounds and Optimal Algorithms for Personalized Federated Learning, joint work with Filip Hanzely, Slavomir Hanzely,and Peter Richtarik, got accepted to the NeurIPS 2020. This year conference is online and it takes place from 6th to 12th December 2020.\n","date":1601078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601078400,"objectID":"eca93e8c798c3593f3bc81d60c908add","permalink":"https://samuelhorvath.github.io/post/neurips2020accepted/","publishdate":"2020-09-26T00:00:00Z","relpermalink":"/post/neurips2020accepted/","section":"post","summary":"Lower Bounds and Optimal Algorithms for Personalized Federated Learning","tags":[],"title":"One paper accepted to NeurIPS 2020","type":"post"},{"authors":["Filip Hanzely","Slavomir Hanzely","Samuel Horvath","Peter Richtarik"],"categories":null,"content":"","date":1600819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600819200,"objectID":"b21a205818b4106eb076789bdfa0843c","permalink":"https://samuelhorvath.github.io/publication/fl_implicit_opt_alg/","publishdate":"2020-09-23T00:00:00Z","relpermalink":"/publication/fl_implicit_opt_alg/","section":"publication","summary":"In this work, we consider the optimization formulation of personalized federated learning recently introduced by Hanzely \u0026 Richtarik (2020) which was shown to give an alternative explanation to the workings of local SGD methods. Our first contribution is establishing the first lower bounds for this formulation, for both the communication complexity and the local oracle complexity. Our second contribution is the design of several optimal methods matching these lower bounds in almost all regimes. These are the first provably optimal methods for personalized federated learning. Our optimal methods include an accelerated variant of FedProx, and an accelerated variance-reduced version of FedAvg/Local SGD. We demonstrate the practical superiority of our methods through extensive numerical experiments.","tags":null,"title":"Lower Bounds and Optimal Algorithms for Personalized Federated Learning","type":"publication"},{"authors":["Samuel Horvath"],"categories":[],"content":"I have joined Samsung AI Centre as Research Intern. I am staying in Cambridge fro September 2020 to February 2021. I am part of Embedded AI Team and I am working under supervision of Nicholas Lanen, Stefanos Laskaridis, and Ilias Leontiadis.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"2c6881585f907ca014e5ac110deb4625","permalink":"https://samuelhorvath.github.io/post/samsung/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/post/samsung/","section":"post","summary":"I have joined Samsung as Research Intern from Sept. 2020 to Feb. 2021.","tags":[],"title":"Samsung AI Centre Cambridge Internship","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I presented our work A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning, joint work with Peter Richtarik at Federated Learning One World Seminar. The one hour recording and slides are available here.\n","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596585600,"objectID":"aecd43489198d8fceeca934d61428aff","permalink":"https://samuelhorvath.github.io/post/flow_talk/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/post/flow_talk/","section":"post","summary":"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning","tags":[],"title":"Talk at Federated Learning One World Seminar","type":"post"},{"authors":["Samuel Horvath","Peter Richtarik"],"categories":null,"content":"","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784000,"objectID":"8fbe35ea5f21c9e705a10543e282cd6c","permalink":"https://samuelhorvath.github.io/publication/euf/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/publication/euf/","section":"publication","summary":"Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-K.  In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.","tags":null,"title":"A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning is now available on the arXiv. This is a joint work with my supervisor Peter Richtarik.\nAbstract:\nModern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-$K$. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.\n","date":1592773200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592773200,"objectID":"b75a05ca109b135f599d0ac40da33468","permalink":"https://samuelhorvath.github.io/post/new_paper_out_uef/","publishdate":"2020-06-22T00:00:00+03:00","relpermalink":"/post/new_paper_out_uef/","section":"post","summary":"[**A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning**](https://arxiv.org/pdf/2006.11077.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I have been accepted to two Machine Learning Summer Schools. MLSS Tuebingen takes place from 28. June to 10. July (acceptance rate 130/1800+) and MLSS Indonesia from 3. to 9. August.\n","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"4801f19f39b604e663c5d33afb3df16b","permalink":"https://samuelhorvath.github.io/post/mlss_2020/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/post/mlss_2020/","section":"post","summary":"MLSS Tuebingen and Indonesia","tags":[],"title":"Summer Schools 2020","type":"post"},{"authors":["Aleksandr Beznosikov","Samuel Horvath","Peter Richtarik","Mher Safaryan"],"categories":null,"content":"","date":1583107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583107200,"objectID":"e389a54a6fcef527b773b7567c93a13d","permalink":"https://samuelhorvath.github.io/publication/biased_compression/","publishdate":"2020-03-02T00:00:00Z","relpermalink":"/publication/biased_compression/","section":"publication","summary":"In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. Further, via a theoretical study of several synthetic and empirical distributions of communicated gradients, we shed light on why and by how much biased compressors outperform their unbiased variants. Finally, we propose a new highly performing biased compressor—combination of Top-k and natural dithering—which in our experiments outperforms all other compression techniques.","tags":null,"title":"On Biased Compression for Distributed Learning","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper On Biased Compression for Distributed Learning is now available on the arXiv. This is a joint work with Aleksandr Beznosikov, Mher Safaryan and my supervisor Peter Richtarik.\nAbstract:\nIn the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. Our distributed SGD method enjoys the ergodic rate $\\mathcal{O}\\left(\\frac{\\delta Le^{-K}}{\\mu} + \\frac{C+D}{K\\mu}\\right)$ \u0011, where $\\delta$ is a compression parameter which grows when more compression is applied, $L$ and $\\mu$ are the smoothness and strong convexity constants, $C$ captures stochastic gradient noise ($C = 0$ if full gradients are computed on each node) and $D$ captures the variance of the gradients at the optimum ($D = 0$ for over-parameterized models). Further, via a theoretical study of several synthetic and empirical distributions of communicated gradients, we shed light on why and by how much biased compressors outperform their unbiased variants. Finally, we propose a new highly performing biased compressor—combination of Top-k and natural dithering—which in our experiments outperforms all other compression techniques.\n","date":1583096400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583096400,"objectID":"699f6a56b18c9448b0a29efc2b43d33b","permalink":"https://samuelhorvath.github.io/post/new_paper_out_biased_compression/","publishdate":"2020-03-02T00:00:00+03:00","relpermalink":"/post/new_paper_out_biased_compression/","section":"post","summary":"[**On Biased Compression for Distributed Learning**](https://arxiv.org/pdf/2002.12410.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath","Lihua Lei","Peter Richtarik","Michael I. Jordan"],"categories":null,"content":"","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"00c0d3b1ef2c5a6e9279ac8e7773f75b","permalink":"https://samuelhorvath.github.io/publication/sc_sarah/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/publication/sc_sarah/","section":"publication","summary":"Adaptivity is an important yet under-studied property in modern optimization theory. The gap between the state-of-the-art theory and the current practice is striking in that algorithms with desirable theoretical guarantees typically involve drastically different settings of hyperparameters, such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results, such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly without tweaking the hyperparameters. In this work, blending the \"geometrization\" technique introduced by Lei \u0026 Jordan, 2016, and the __SARAH__ algorithm of Nguyen et al., we propose the Geometrized  __SARAH__ algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity to both the magnitude of the target accuracy and the Polyak-Lojasiewicz (PL) constant, if present. In addition, it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming existing algorithms for PL objectives.","tags":null,"title":"Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper Adaptivity of Stochhastic Gradient Methods for Nonconvex Optimization is now available on the arxiv. This project originated in May 2019 while I was visiting prof. Michael I. Jordan at Berkeley and is the joint work with Lihua Lei, that time PhD student at Berkeley, and my supervisor Peter Richtarik.\nAbstract:\nAdaptivity is an important yet under-studied property in modern optimization theory. The gap between the state-of-the-art theory and the current practice is striking in that algorithms with desirable theoretical guarantees typically involve drastically different settings of hyperparameters, such as step-size schemes and batch sizes, in different regimes. Despite the appealing theoretical results, such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly without tweaking the hyperparameters. In this work, blending the \u0026ldquo;geometrization\u0026rdquo; technique introduced by Lei \u0026amp; Jordan, 2016, and the SARAH algorithm of Nguyen et al., we propose the Geometrized SARAH algorithm for non-convex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity to both the magnitude of the target accuracy and the Polyak-Lojasiewicz (PL) constant, if present. In addition, it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming existing algorithms for PL objectives.\n","date":1581454800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581454800,"objectID":"c3a326954b12f2e5ddfd3386ff5ac6dd","permalink":"https://samuelhorvath.github.io/post/new_paper_out_berkeley/","publishdate":"2020-02-12T00:00:00+03:00","relpermalink":"/post/new_paper_out_berkeley/","section":"post","summary":"[**Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization**](https://arxiv.org/pdf/2002.05359.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I attended The 31st International Conference on Algorithmic Learning Theory in San Diego. I presented our accepted paper SVRG and Katyusha are Better without the Outer Loop as 20 minutes talk.\n","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"389c4c64b398791bac0a7db2629f054f","permalink":"https://samuelhorvath.github.io/post/alt_2020_conf/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/alt_2020_conf/","section":"post","summary":"Catamaran Hotel Resort, San Diego.","tags":[],"title":"Attending ALT 2020","type":"post"},{"authors":[],"categories":null,"content":"Our paper Don’t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop, joint work with Dmitry Kovalev and Peter Richtarik, got accepted to the ALT 2020. The conference takes place from 8-11th February at San Diego, California.\nAbstract:\nThe stochastic variance-reduced gradient method (SVRG) and its accelerated variant (Katyusha) have attracted enormous attention in the machine learning community in the last few years due to their superior theoretical properties and empirical behaviour on training supervised machine learning models via the empirical risk minimization paradigm. A key structural element in both of these methods is the inclusion of an outer loop at the beginning of which a full pass over the training data is made in order to compute the exact gradient, which is then used in an inner loop to construct a variance-reduced estimator of the gradient using new stochastic gradient information. In this work we design loopless variants of both of these methods. In particular, we remove the outer loop and replace its function by a coin flip performed in each iteration designed to trigger, with a small probability , the computation of the gradient. We prove that the new methods enjoy the same superior theoretical convergence properties as the original methods. For loopless SVRG, the same rate is obtained for a large interval of coin flip probabilities, including the probability $\\frac{1}{n}$, where $n$ is the number of functions. This is the first result where a variant of SVRG is shown to converge with the same rate without the need for the algorithm to know the condition number, which is often unknown or hard to estimate correctly. We demonstrate through numerical experiments that the loopless methods can have superior and more robust practical behavior.\n","date":1568926800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568926800,"objectID":"4ce135ce7b39ce3c7b84a8c759652990","permalink":"https://samuelhorvath.github.io/post/alt_2020_paper/","publishdate":"2019-09-20T00:00:00+03:00","relpermalink":"/post/alt_2020_paper/","section":"post","summary":"We get one paper accepted!","tags":[],"title":"ALT 2020","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I attended ICCOPT 2019, the Sixth International Conference on Continuous Optimization, which took place on the campus of the Technical University (TU) of Berlin, August 3-8, 2019. The ICCOPT is a flagship conference of the Mathematical Optimization Society (MOS), organized every three years. I organized a minisymposia together with Filip Hanzely there and gave a talk about our work Stochastic Distributed Learning with Gradient Quantization and Variance Reduction.\n","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"3f78fff68389e4540b8dce9e129e64d2","permalink":"https://samuelhorvath.github.io/post/iccopt_2019/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/iccopt_2019/","section":"post","summary":"TU Berlin, Germany.","tags":[],"title":"ICCOPT 2019","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I have joined Amazon as Applied Scientist Intern for Summer 2019. I am part of AI Core Team supervised by Cedric Archambeau and Matthias Seeger.\nUPDATE I finished my internship. While being at Amazon, I had pleasure to attend Amazon EMEA Research Internship Colloquium, the conference for all of Amazon interns from EMEA region, where I presented a poster based on our paper Natural Compression for Distributed Deep Learning. In addition, I attended Amazon Research Days, the conference focused on promoting the collaboration with academia. In terms of research, I was working on scalable transfer learning for hyperparameter optimization, closely working with Cedric Archambeau, Aaron Klein, and Valerio Perrone. I hope our work will be available online soon.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"0d3d425284119bfb88001ae91d9451b1","permalink":"https://samuelhorvath.github.io/post/amazon/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/post/amazon/","section":"post","summary":"I have joined Amazon as Applied Science Intern for Summer 2019.","tags":[],"title":"Amazon Research Internship","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I attended Thirty-sixth International Conference on Machine Learning, where we had our work Nonconvex Variance Reduced Optimization with Arbitrary Sampling accepted, which I presented as the 5 min talk as well as the poster.\n","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"16f2f48c4d674a67112eb54f9bbcb158","permalink":"https://samuelhorvath.github.io/post/icml_2019_conf/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/post/icml_2019_conf/","section":"post","summary":"Long Beach Convention Center, Long Beach.","tags":[],"title":"Attending ICML 2019","type":"post"},{"authors":["Samuel Horvath","Chen-Yu Ho","Ludovit Horvath","Atal Narayan Sahu","Marco Canini","Peter Richtarik"],"categories":null,"content":"","date":1558915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558915200,"objectID":"f4ffec458272ef77e3e791752a8dd295","permalink":"https://samuelhorvath.github.io/publication/intml/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/publication/intml/","section":"publication","summary":"Due to their hunger for big data, modern deep learning models are trained in parallel, often in distributed environments, where communication of model updates is the bottleneck. Various update compression (e.g., quantization, sparsification, dithering) techniques have been proposed in recent years as a successful tool to alleviate this problem. In this work, we introduce a new, remarkably simple and theoretically and practically effective compression technique, which we call *natural compression* _NC_. Our technique is applied individually to all entries of the to-be-compressed update vector and works by randomized rounding to the nearest (negative or positive) power of two. _NC_ is \"natural\" since the nearest power of two of a real expressed as a float can be obtained without any computation, simply by ignoring the mantissa. We show that compared to no compression,  _NC_ increases the second moment of the compressed vector by the tiny factor  9/8 only, which means that the effect of _NC_ on the convergence speed of popular training algorithms, such as distributed SGD, is negligible.  However, the communications savings enabled by _NC_ are substantial, leading to  _3-4x_ improvement in overall theoretical running time}. For applications requiring more aggressive compression, we generalize _NC_ to _natural dithering_, which we prove is **exponentially better** than the immensely popular random dithering technique. Our compression operators can be used on their own or in combination with existing operators for a more aggressive combined effect. Finally, we show that _NC_ is particularly effective for the in-network aggregation (INA) framework for distributed training, where the update aggregation is done on a switch, which can only perform integer computations.","tags":null,"title":"Natural Compression for Distributed Deep Learning","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper Natural Compression for Distributed Deep Learning is now available on the arxiv. This is the joint work with Chen-Yu Ho, Ludovit Horvath, Atal Sahu, Marco Canini and Peter Richtarik.\nAbstract:\nDue to their hunger for big data, modern deep learning models are trained in parallel, often in distributed environments, where communication of model updates is the bottleneck. Various update compression (e.g., quantization, sparsification, dithering) techniques have been proposed in recent years as a successful tool to alleviate this problem. In this work, we introduce a new, remarkably simple and theoretically and practically effective compression technique, which we call natural compression $\\mathcal{NC}$. Our technique is applied individually to all entries of the to-be-compressed update vector and works by randomized rounding to the nearest (negative or positive) power of two. $\\mathcal{NC}$ is \u0026ldquo;natural\u0026rdquo; since the nearest power of two of a real expressed as a float can be obtained without any computation, simply by ignoring the mantissa. We show that compared to no compression, $\\mathcal{NC}$ increases the second moment of the compressed vector by the tiny factor $\\frac{9}{8}$ only, which means that the effect of $\\mathcal{NC}$ on the convergence speed of popular training algorithms, such as distributed SGD, is negligible. However, the communications savings enabled by $\\mathcal{NC}$ are substantial, leading to $3$-$4$x improvement in overall theoretical running time}. For applications requiring more aggressive compression, we generalize $\\mathcal{NC}$ to natural dithering, which we prove is exponentially better than the immensely popular random dithering technique. Our compression operators can be used on their own or in combination with existing operators for a more aggressive combined effect. Finally, we show that $\\mathcal{NC}$ is particularly effective for the in-network aggregation (INA) framework for distributed training, where the update aggregation is done on a switch, which can only perform integer computations.\n","date":1558904400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558904400,"objectID":"cb8a6c89be9148f1bab2dddf1ae28832","permalink":"https://samuelhorvath.github.io/post/new_paper_out_intml/","publishdate":"2019-05-27T00:00:00+03:00","relpermalink":"/post/new_paper_out_intml/","section":"post","summary":"[**Natural Compression for Distributed Deep Learning**](https://arxiv.org/pdf/1905.10988.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath","Peter Richtarik"],"categories":null,"content":"","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"d7a1fdeff6525ff8da08b028f8044358","permalink":"https://samuelhorvath.github.io/publication/non_unif/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/non_unif/","section":"publication","summary":"We provide the first importance sampling variants of variance-reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of SVRG, SAGA and SARAH. Our methods have the capacity to speed up the training process by an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing importance sampling for minibatches in this setting. Ours are the first optimal samplings for minibatches in the literature on stochastic optimization. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with arbitrary sampling, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of SARAH in the convex setting.","tags":null,"title":"Nonconvex Variance Reduced Optimization with Arbitrary Sampling","type":"publication"},{"authors":["Samuel Horvath"],"categories":[],"content":"I am at Berkeley, where I am visiting Michael I. Jordan. I will stay here for a month and then I am going to Los Angeles to attend ICML, where I will present our work. During my stay at Berkeley, I will also attend Deep Learning Bootcamp at Simons Institute.\n","date":1557792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557792000,"objectID":"cceec3cbf276760fa87b91a2741f99db","permalink":"https://samuelhorvath.github.io/post/berkeley/","publishdate":"2019-05-14T00:00:00Z","relpermalink":"/post/berkeley/","section":"post","summary":"I am visiting [**Michael I. Jordan**](https://people.eecs.berkeley.edu/~jordan/).","tags":[],"title":"Berkeley Research Visit","type":"post"},{"authors":[],"categories":null,"content":"Our paper Nonconvex Variance Reduced Optimization with Arbitrary Sampling, joint work with Peter Richtarik, got accepted to the ICML 2019. The conference takes place from 9-15th June at Long Beach, California.\nAbstract:\nWe provide the first importance sampling variants of variance reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of SVRG, SAGA and SARAH. Our methods have the capacity to speed up the training process by an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing importance sampling for minibatches in this setting. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with arbitrary sampling, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of SARAH in the convex setting.\nPoster:\n","date":1557435600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557435600,"objectID":"f1c99e711e10c29896ca01df2b45e0e7","permalink":"https://samuelhorvath.github.io/post/icml_2019_paper/","publishdate":"2019-05-10T00:00:00+03:00","relpermalink":"/post/icml_2019_paper/","section":"post","summary":"We get one paper accepted!","tags":[],"title":"ICML 2019","type":"post"},{"authors":[],"categories":null,"content":"Our new paper Stochastic Distributed Learning with Gradient Quantization and Variance Reduction will be soon available on the arxiv. This is the joint work with Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and my supervisor Peter Richtarik.\nAbstract:\nWe consider distributed optimization where the objective function is spread among different devices, each sending incremental model updates to a central server. To alleviate the communication bottleneck, recent work proposed various schemes to compress (e.g.\\ quantize or sparsify) the gradients, thereby introducing additional variance $\\omega \\geq 1$ that might slow down convergence. For strongly convex functions with condition number $\\kappa$ distributed among $n$ machines:\n  we give a scheme that converges in $\\mathcal{O}((\\kappa + \\kappa \\frac{\\omega}{n} + \\omega)$$\\log (1/\\epsilon))$ steps to a neighborhood of the optimal solution. For objective functions with a finite-sum structure, each worker having less than $m$ components,\n  we present novel variance reduced schemes that converge in $\\mathcal{O}((\\kappa + \\kappa \\frac{\\omega}{n} + \\omega + m)\\log(1/\\epsilon))$ steps to arbitrary accuracy $\\epsilon \u0026gt; 0$. These are the first methods that achieve linear convergence for arbitrary quantized updates,\n  we give analysis for the weakly convex and non-convex cases,\n  we verify in experiments that our novel variance reduced schemes are more efficient than the baselines.\n  ","date":1554670800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554670800,"objectID":"0d968528fdc66fab764eaa861f5acf29","permalink":"https://samuelhorvath.github.io/post/new_paper_out_quant_var/","publishdate":"2019-04-08T00:00:00+03:00","relpermalink":"/post/new_paper_out_quant_var/","section":"post","summary":"[**Stochastic Distributed Learning with Gradient Quantization and Variance Reduction**](https://arxiv.org/pdf/1904.05115.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath","Dmitry Kovalev","Konstantin Mishchenko","Sebastian Stich","Peter Richtarik"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"1288331f32f1f677c0aaf963ef38cc95","permalink":"https://samuelhorvath.github.io/publication/diana2/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/diana2/","section":"publication","summary":"We consider distributed optimization where the objective function is spread among different devices, each sending incremental model updates to a central server. To alleviate the communication bottleneck, recent work proposed various schemes to compress (e.g. quantize or sparsify) the gradients, thereby introducing additional variance hat might slow down convergence. For strongly convex functions distributed among n machines,  we give a scheme that converges linearly to a neighborhood of the optimal solution. For objective functions with a finite-sum structure, each worker having less than m components, we present novel variance reduced schemes that converge linearly to arbitrary accuracy. These are the first methods that achieve linear convergence for arbitrary quantized updates. We also give analysis for the weakly convex and non-convex cases and we verify in experiments that our novel variance reduced schemes are more efficient than the baselines.","tags":null,"title":"Stochastic Distributed Learning with Gradient Quantization and Variance Reduction","type":"publication"},{"authors":[],"categories":null,"content":"Our new paper Don\u0026rsquo;t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop is now available on the arxiv. This is the joint work with Dmitry Kovalev and my supervisor Peter Richtarik.\nAbstract:\nThe stochastic variance-reduced gradient method (SVRG) and its accelerated variant (Katyusha) have attracted enormous attention in the machine learning community in the last few years due to their superior theoretical properties and empirical behaviour on training supervised machine learning models via the empirical risk minimization paradigm. A key structural element in both of these methods is the inclusion of an outer loop at the beginning of which a full pass over the training data is made in order to compute the exact gradient, which is then used to construct a variance-reduced estimator of the gradient. In this work we design loopless variants of both of these methods. In particular, we remove the outer loop and replace its function by a coin flip performed in each iteration designed to trigger, with a small probability, the computation of the gradient. We prove that the new methods enjoy the same superior theoretical convergence properties as the original methods. However, we demonstrate through numerical experiments that our methods have substantially superior practical behavior.\n","date":1548277200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548277200,"objectID":"7dc8815a3a646e357bb90905dd28dc07","permalink":"https://samuelhorvath.github.io/post/new_paper_out_stupid/","publishdate":"2019-01-24T00:00:00+03:00","relpermalink":"/post/new_paper_out_stupid/","section":"post","summary":"[**Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop**](https://arxiv.org/pdf/1901.08689.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Dmitry Kovalev","Samuel Horvath","Peter Richtarik"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"94a69cdde1a27e016942dc2626317916","permalink":"https://samuelhorvath.github.io/publication/stupid/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/stupid/","section":"publication","summary":"The stochastic variance-reduced gradient method (*SVRG*) and its accelerated variant (*Katyusha*) have attracted enormous attention in the machine learning community in the last few years due to their superior theoretical properties and empirical behaviour on training supervised machine learning models via the empirical risk minimization paradigm. A key structural element in both of these methods is the inclusion of an outer loop at the beginning of which  a full pass over the training data is made in order to compute the exact gradient, which is then used to construct a variance-reduced estimator of the gradient. In this work we design _loopless variants_ of both of these methods. In particular,  we remove the outer loop and replace its function by a coin flip performed in each iteration designed to trigger, with a small probability, the computation of the gradient. We prove that the new methods enjoy the same superior theoretical convergence properties as the original methods. However, we demonstrate through numerical experiments that our methods have substantially superior practical behavior.","tags":null,"title":"Don’t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop","type":"publication"},{"authors":["Samuel Horvath"],"categories":[],"content":"After 3 months, I finished all five projects of Deep Learning Nanodegree consisting of Neural Nets, CNN, RNN, GANs and Deep Reinforcement Learning. This course is taught by well-known AI experts such as Sebastian Thrun, Ian Goodfellow and Andrew Trask.\n","date":1536969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536969600,"objectID":"09e1175cd269664353853884f90050f5","permalink":"https://samuelhorvath.github.io/post/udacity/","publishdate":"2018-09-15T00:00:00Z","relpermalink":"/post/udacity/","section":"post","summary":"successfully graduated","tags":[],"title":"Deep Learning Nanodegree","type":"post"},{"authors":[],"categories":null,"content":"Our paper is now available on the arxiv. A poster based on the results of this paper won the Best Poster at Data Science Summer School in Paris, 2018.\nAbstract:\nWe provide the first importance sampling variants of variance reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of SVRG, SAGA and SARAH. Our methods have the capacity to speed up the training process by an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing importance sampling for minibatches in this setting. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with arbitrary sampling, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of SARAH in the convex setting.\nPoster:\n","date":1536613200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536613200,"objectID":"517dcb1e768f8221e7ce2f4186cc231f","permalink":"https://samuelhorvath.github.io/post/new_paper_out_sampling/","publishdate":"2018-09-11T00:00:00+03:00","relpermalink":"/post/new_paper_out_sampling/","section":"post","summary":"[**Nonconvex Variance Reduced Optimization with Arbitrary Sampling**](https://arxiv.org/pdf/1809.04146.pdf)","tags":[],"title":"New paper out","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I am attending Data Science Summer School in Paris at École Polytechnique. I will present there poster with title Nonconvex Variance Reduced Optimization with Arbitrary Sampling, which is based on a paper of the same title, joint work with my supervisor Peter Richtárik, and currently under review.\n Update   My poster was awarded as $\\text{the Best DS}^3\\text{poster}$ with $500$ Euros cash prize. Only 2 posters out of total 170 were awarded!\n","date":1529798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529798400,"objectID":"9a47d7c70f5a9acfbd562648b9c6e11c","permalink":"https://samuelhorvath.github.io/post/dss_2018/","publishdate":"2018-06-24T00:00:00Z","relpermalink":"/post/dss_2018/","section":"post","summary":"My poster was awarded as the [**best poster**](http://www.ds3-datascience-polytechnique.fr/posters/) among **170** presented posters.","tags":[],"title":"Data Science Summer School Paris","type":"post"},{"authors":["Samuel Horvath"],"categories":[],"content":"I am joining Exponea as AI Intern for Summer 2018. I will join their Recommendation Team, where I will work on Sorting and Ranking Project for personalized recommendation for e-commerce.\n","date":1528070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528070400,"objectID":"140f776db2d41ed03088e7170ee34dce","permalink":"https://samuelhorvath.github.io/post/exponea/","publishdate":"2018-06-04T00:00:00Z","relpermalink":"/post/exponea/","section":"post","summary":"I am joining [Exponea](https://exponea.com/) as AI Intern for Summer 2018.","tags":[],"title":"Exponea AI Internship","type":"post"}]